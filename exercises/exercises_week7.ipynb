{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "The exercises today are about extracting high-level knowledge from text. We're still a long way from computers being able to give us insight as deep as that which we can aquire from manually reading text, but some the tools that you will use today get us a long way in understanding useful things about unreadibly large amount of text in comparatively little time. In the exercises today you will:\n",
    "\n",
    "* Create wordclouds\n",
    "* Extract sentiment from text\n",
    "* Construct a Bag of Words (BoW) matrix to represent how words are used about each faction in the Marvel dataset\n",
    "* Perform a TD-IDF transform to understand which words are important to different characters\n",
    "\n",
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 (Extra): Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although they probably offer more buzz than usefulness, wordclouds are a fun way to get quick insight into which words are used in a corpus of text. In this section you will generate one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 7.1.1**: To make a word cloud you need some more or less clean text. For each character extract as cleanly as you can, the text written on their wikipage. Since the wikidata is kind of messy, there are some things you should be aware of when extracting the text.\n",
    "* Exclude character names\n",
    "* Exclude links\n",
    "* Exclude references\n",
    "* Exclude numbers\n",
    "* Set everything to lower case\n",
    "* Do not include stopwords (use `nltk.corpus.stopwords.words(\"english\")` to get a list of stopwords; install `nltk` to do this)\n",
    "\n",
    ">Cleaning doesn't have to be perfect and can be done in a many different ways, these are just some things to look out for.\n",
    "\n",
    ">Once you have extracted the text, create one long text string for all text written about heroes, another long text string for villains, and finally one for ambiguous characters. Using the code snippet below which shows how to plot a word cloud, plot the word clouds for each faction.\n",
    "\n",
    ">        text = \"some cool text\"\n",
    ">        wc = wordcloud.WordCloud(max_font_size=40).generate(text)\n",
    ">        \n",
    ">        plt.figure()\n",
    ">        plt.imshow(wc, interpolation=\"bilinear\")\n",
    ">        plt.axis(\"off\")\n",
    ">        plt.show()\n",
    "\n",
    "> You have to import `matplotlib.pylab` and `wordcloud` to do this. You can install `wordcloud` with anaconda by typing into your console\n",
    "\n",
    ">        conda install -c conda-forge wordcloud\n",
    "\n",
    ">or\n",
    "\n",
    ">        pip install wordcloud\n",
    "\n",
    ">if you don't have Anaconda installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to get vector of words for a character. Make sure the path to your data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T12:43:17.770171Z",
     "start_time": "2019-10-22T12:43:17.659070Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "# Get a list of stopwords from nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def get_clean_words(character_filename, faction):\n",
    "    def _isnum(w):\n",
    "        try:\n",
    "            int(w)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "        \n",
    "    # Load her markup\n",
    "    with open(\"../data/%s/%s\" % (faction, character_filename)) as fp:\n",
    "        markup = fp.read()\n",
    "\n",
    "    # Remove table and external links\n",
    "    markup_text = re.sub(r'\\{\\{[\\s\\S]*?\\}\\}', '', markup)\n",
    "\n",
    "    # Remove category links\n",
    "    markup_text = re.sub(r'\\[\\[Category.+\\]\\]', '', markup_text)\n",
    "\n",
    "    # Set words to lowercase and remove them if they are stop words\n",
    "    words = [w.lower() for w in re.findall('\\w+', markup_text) if w.lower() not in stopwords]\n",
    "\n",
    "    # Remove numbers\n",
    "    words = [w for w in words if not _isnum(w)]\n",
    "\n",
    "    return words\n",
    "\n",
    "# Example\n",
    "get_clean_words(\"Iron Man.txt\", \"heroes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you don't have time to read some text, but you need to know something about whether it positively or negatively toned. Enter *Sentiment Analysis*. The point of this exercise is to extract the sentiment of text on your heroes, villains and ambiguous characters and figure out whether Wikipedia is biased towards writing in a certain tone towards a certain kind of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.2.1**: Compute the sentiment score of each character's page and produce three histograms of sentiment scores, one for each faction. You can use the text strings you generated in the previous exercise. We will cheat a bit and use a library that does the scoring for us. Install `afinn` using `conda` or `pip` and extract the sentiment with that module. There's an example of how to use it on the library's [PyPi repository](https://pypi.python.org/pypi/afinn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Bag of Words matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, \"Bag of Words\" means breaking up a document into words and throwing them into a bag. And that's very close to the truth! In week 5 you constructed a \"team-affiliations\" matrix which had a row for each character and a column for each team. If the character was on a given team there would be a one for that character row at that team column, if not there would be a zero. The BoW is the same, only now, rather than teams, your columns are individual words that a character's wikipage might contain, and the numbers represent how many times those words appear.\n",
    "<img src=\"http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_bow.png\" width=\"400\"/>\n",
    "BoW's are pretty large and sparse (mostly contain zero's) matrices, but they are extremely useful because they allow us to use linear algebra to do things like PCA, classification, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 7.3.1**: Create a bag of words matrix that contains all your characters. Maintain also a target array, so you know whether a row corresponds to a hero, a villain or an ambiguous character. Do not include stopwords like \"is\", \"a\", etc. (get list of stopwords from `nltk.corpus.stopwords.words(\"english\")`). Also, do not include words that only appear for one character.\n",
    "1. How many different words are in your vocabulary/columns are in your matrix?\n",
    "2. Print the 10 most used words, and the 10 least used words, along with their usage count.\n",
    "3. Plot the distribution of how many times words are used.\n",
    "\n",
    ">*Hint: Since you already extracted lists of words for each faction in Ex. 7.1.1, you can use these to figure out what the total vocabulary of words used in your dataset is. You can \"clean up\" this vocabulary by a number of tricks. For example, there are tools for *stemming* words to remove grammar so that e.g. 'cat' and 'cats' both become 'cat', but that's all up to you whether you wanna go that deep.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.3.EXTRA**: Plot the first two components of the PCA transform of your BoW matrix. See anything interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.3.EXTRA**: Create a classifier that predicts if a character is a hero or a villain from the words used on their page. Report its 10-fold cross validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You didn't just make that BoW matrix to count simple things. We are interested in knowing how (or if) words are used differently across characters, and the best way to do that is to used something called a Term Frequency - Inverse Document Frequency (TF-IDF) transformation. You can read about it [on Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), but the gist is that it reflects how important each word is to each document (a document in this case being a faction).\n",
    "\n",
    "It works in two steps:\n",
    "* (1 - TF) you normalize over the frequency of each word in each document, so that rows sum to 1. Every row is now a probability distribution (a pmf to be exact) that gives the \"term frequency\" in each document.\n",
    "* (2 - IDF) you weigh the TF by the inverse document frequency, which measures how unique a word to specific documents. For example, the word \"the\" will be frequently used in every document (high TF) but we know it's not very special because it's used in all documents so the inverse document frequency is low, yielding a vanishing TF-IDF score for \"the\" in all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 7.4.1**: Use any tool you like (you can do it manually, it's straight forward if you understand the method), to perform a TF-IDF transform on your BoW matrix from Ex. 7.3.1. The result should be a matrix of the same shape as the BoW, but with different values inside.\n",
    "1. Explain what these values mean.\n",
    "2. For the top three most written about characters in each class (so 9 in total), print out each of their 10 highest scoring words. Comment on any differences you observe in the type of words being used in different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.4.2**: Import `from sklearn.manifold import TSNE` and transform the BoW matrix into a lower-dimensional space. Plot the points in this space.\n",
    ">\n",
    ">*Hint: You can simply do `X_tsne = TSNE(n_components=2).fit_transform(X_BoW)` and then plot the 0th and 1st axis of `X_tsne` against each other. If there are clusters of words use in the data, this plot will show them. You can make this even cooler by coloring the scattered points by their class.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
